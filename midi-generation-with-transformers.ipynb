{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIDI generation with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T05:54:23.831789Z",
     "iopub.status.busy": "2025-06-21T05:54:23.831486Z",
     "iopub.status.idle": "2025-06-21T05:54:32.395516Z",
     "shell.execute_reply": "2025-06-21T05:54:32.394465Z",
     "shell.execute_reply.started": "2025-06-21T05:54:23.831761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0.dev20250404+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: pretty_midi in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.10)\n",
      "Requirement already satisfied: miditoolkit in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: mido>=1.1.16 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretty_midi) (1.3.3)\n",
      "Requirement already satisfied: six in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretty_midi) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexander\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch numpy matplotlib pretty_midi miditoolkit tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T05:54:32.397140Z",
     "iopub.status.busy": "2025-06-21T05:54:32.396855Z",
     "iopub.status.idle": "2025-06-21T05:54:38.082802Z",
     "shell.execute_reply": "2025-06-21T05:54:38.082026Z",
     "shell.execute_reply.started": "2025-06-21T05:54:32.397117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from miditoolkit import MidiFile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pretty_midi\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T05:54:38.085075Z",
     "iopub.status.busy": "2025-06-21T05:54:38.084659Z",
     "iopub.status.idle": "2025-06-21T05:54:38.088567Z",
     "shell.execute_reply": "2025-06-21T05:54:38.087924Z",
     "shell.execute_reply.started": "2025-06-21T05:54:38.085051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"clean_midi\"\n",
    "OUTPUT_PATH = \"generated_midi\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T05:54:38.089901Z",
     "iopub.status.busy": "2025-06-21T05:54:38.089679Z",
     "iopub.status.idle": "2025-06-21T05:54:38.107807Z",
     "shell.execute_reply": "2025-06-21T05:54:38.107053Z",
     "shell.execute_reply.started": "2025-06-21T05:54:38.089882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Embeddings size\n",
    "d_model = 256\n",
    "\n",
    "# Attention heads number\n",
    "n_heads = 8\n",
    "\n",
    "# Transformer layers number\n",
    "num_layers = 6\n",
    "\n",
    "# Feed-forward layer size\n",
    "d_ff = 4096\n",
    "\n",
    "# Dropout\n",
    "dropout = 0.1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T05:54:38.109057Z",
     "iopub.status.busy": "2025-06-21T05:54:38.108721Z",
     "iopub.status.idle": "2025-06-21T05:54:38.124185Z",
     "shell.execute_reply": "2025-06-21T05:54:38.123645Z",
     "shell.execute_reply.started": "2025-06-21T05:54:38.109028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "seq_len = 256 # Length of a sequence\n",
    "epochs = 20\n",
    "learning_rate = 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the MIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T05:54:38.125131Z",
     "iopub.status.busy": "2025-06-21T05:54:38.124840Z",
     "iopub.status.idle": "2025-06-21T05:54:38.151985Z",
     "shell.execute_reply": "2025-06-21T05:54:38.151235Z",
     "shell.execute_reply.started": "2025-06-21T05:54:38.125104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 7938\n"
     ]
    }
   ],
   "source": [
    "class MIDITokenizer:\n",
    "    def __init__(self):\n",
    "        self.event2idx = {}\n",
    "        self.idx2event = {}\n",
    "        self._build_vocab()\n",
    "\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        # Events: notes, pauses, control events\n",
    "        note_events = [f'NOTE_{i}' for i in range(128)]\n",
    "        velocity_events = [f'VELOCITY_{i}' for i in range(128)]\n",
    "        duration_events = [f'DURATION_{i}' for i in range(1, 7681)] # Ticks is in interval of 1--128\n",
    "        control_events = [f'TIME_SHIFT', 'END_OF_TRACK']\n",
    "\n",
    "        # Collecting all events\n",
    "        all_events = note_events + velocity_events + duration_events + control_events\n",
    "\n",
    "        # Create dicts\n",
    "        self.event2idx = {event: idx for idx, event in enumerate(all_events)}\n",
    "        self.idx2event = {idx: event for idx, event in enumerate(all_events)}\n",
    "\n",
    "        self.vocab_size = len(all_events)\n",
    "\n",
    "\n",
    "    def encode(self, midi_file):\n",
    "        # Upload a .mid file\n",
    "        midi = MidiFile(midi_file)\n",
    "        notes = midi.instruments[0].notes\n",
    "\n",
    "        # Sort notes by start time\n",
    "        notes.sort(key=lambda x: (x.start, x.pitch))\n",
    "\n",
    "        events = []\n",
    "        current_time = 0\n",
    "\n",
    "        for note in notes:\n",
    "            # Time between previous and current events\n",
    "            time_shift = note.start - current_time\n",
    "            if time_shift > 0:\n",
    "                events.append('TIME_SHIFT')\n",
    "        \n",
    "            # Add note, velocity & duration\n",
    "            events.append(f'NOTE_{note.pitch}')\n",
    "            events.append(f'VELOCITY_{note.velocity}')\n",
    "            events.append(f'DURATION_{note.duration}')\n",
    "\n",
    "            current_time = note.start\n",
    "\n",
    "        events.append('END_OF_TRACK')\n",
    "\n",
    "        result = []\n",
    "        for event in events:\n",
    "            result.append(self.event2idx[event])\n",
    "        return result\n",
    "\n",
    "\n",
    "    def decode(self, indices):\n",
    "        events = [self.idx2event[event] for event in indices]\n",
    "\n",
    "        # Create new .mid file\n",
    "        midi = pretty_midi.PrettyMIDI()\n",
    "        instrument = pretty_midi.Instrument(program=0)\n",
    "\n",
    "        current_time = 0\n",
    "        current_note = None\n",
    "        current_velocity = None\n",
    "\n",
    "        for event in events:\n",
    "            if event.startswith('NOTE_'):\n",
    "                current_note = int(event.split('_')[1])\n",
    "            elif event.startswith('VELOCITY_'):\n",
    "                current_velocity = int(event.split('_')[1])\n",
    "            elif event.startswith('DURATION_'):\n",
    "                if current_note is not None and current_velocity is not None:\n",
    "                    duration = int(event.split('_')[1])\n",
    "                    note = pretty_midi.Note(\n",
    "                        velocity=current_velocity,\n",
    "                        pitch=current_note,\n",
    "                        start=current_time,\n",
    "                        end=current_time + duration/100\n",
    "                    )\n",
    "                    instrument.notes.append(note)\n",
    "                    current_time += duration/100\n",
    "                    current_note = None\n",
    "                    current_velocity = None\n",
    "            elif event == 'TIME_SHIFT':\n",
    "                current_time += 0.1 # Fixed time shift\n",
    "\n",
    "        midi.instruments.append(instrument)\n",
    "        return midi\n",
    "\n",
    "tokenizer = MIDITokenizer()\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tokenizer, 'tokenizer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T05:54:38.153144Z",
     "iopub.status.busy": "2025-06-21T05:54:38.152873Z",
     "iopub.status.idle": "2025-06-21T06:48:59.221433Z",
     "shell.execute_reply": "2025-06-21T06:48:59.220551Z",
     "shell.execute_reply.started": "2025-06-21T05:54:38.153124Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m dataset = MIDIDataset(DATA_PATH, tokenizer, seq_len)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m dataloader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:385\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    387\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "class MIDIDataset(Dataset):\n",
    "    def __init__(self, dataset_path, tokenizer, seq_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.file_paths = []\n",
    "\n",
    "        # Collect all .mid (.midi) files, skip everything else\n",
    "        for root, _, files in os.walk(DATA_PATH):\n",
    "            for file in files:\n",
    "                if file.endswith('.mid') or file.endswith('.midi'):\n",
    "                    self.file_paths.append(os.path.join(root, file))\n",
    "\n",
    "        # Tokenize all collected files\n",
    "        self.sequences = []\n",
    "        for path in tqdm(self.file_paths[:150]): # Последние 150\n",
    "            try:\n",
    "                tokens = self.tokenizer.encode(path)\n",
    "                # Split into fixed length sequences\n",
    "                for i in range(0, len(tokens) - seq_len, seq_len//2):\n",
    "                    self.sequences.append(tokens[i:i+seq_len])\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                continue\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        # Input sequence and shifted by 1 target sequence\n",
    "        x = torch.tensor(sequence[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(sequence[1:], dtype=torch.long)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dataset = MIDIDataset(DATA_PATH, tokenizer, seq_len)\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoder and Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T06:52:24.628029Z",
     "iopub.status.busy": "2025-06-21T06:52:24.627657Z",
     "iopub.status.idle": "2025-06-21T06:52:24.803252Z",
     "shell.execute_reply": "2025-06-21T06:52:24.802480Z",
     "shell.execute_reply.started": "2025-06-21T06:52:24.628000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 11,962,626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        output = self.transformer(src, src_mask, src_key_padding_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "model = MusicTransformer(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout\n",
    ").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate MIDI file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T06:52:28.946333Z",
     "iopub.status.busy": "2025-06-21T06:52:28.946043Z",
     "iopub.status.idle": "2025-06-21T06:52:32.064965Z",
     "shell.execute_reply": "2025-06-21T06:52:32.064189Z",
     "shell.execute_reply.started": "2025-06-21T06:52:28.946311Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated MIDI saved to generated_midi\\generated_0.7.mid\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(model, tokenizer, temperature=1.0, max_len=512):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Random note as starting token\n",
    "        current_token = torch.tensor([[np.random.randint(0, 128)]], device=model.device)\n",
    "        generated = [current_token.item()]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            # Убираем лишнюю размерность (unsqueeze(0) уже есть в создании тензора)\n",
    "            output = model(current_token)  # теперь форма [1, 1]\n",
    "            \n",
    "            # Получаем последний выход (форма [1, vocab_size])\n",
    "            output = output[:, -1, :] / temperature\n",
    "            probs = torch.softmax(output, dim=-1)\n",
    "            \n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated.append(next_token.item())\n",
    "            \n",
    "            if next_token.item() == tokenizer.event2idx['END_OF_TRACK']:\n",
    "                break\n",
    "                \n",
    "            current_token = next_token\n",
    "        \n",
    "        # Decode & save as file\n",
    "        midi = tokenizer.decode(generated)\n",
    "        output_path = os.path.join(OUTPUT_PATH, f\"generated_{temperature}.mid\")\n",
    "        midi.write(output_path)\n",
    "        print(f\"Generated MIDI saved to {output_path}\")\n",
    "\n",
    "generate_sample(model, tokenizer, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T06:52:36.129326Z",
     "iopub.status.busy": "2025-06-21T06:52:36.128901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [01:36<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.5032\n",
      "Generated MIDI saved to generated_midi\\generated_0.8.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [01:26<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 2.1806\n",
      "Generated MIDI saved to generated_midi\\generated_0.8.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [02:03<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 1.9900\n",
      "Generated MIDI saved to generated_midi\\generated_0.8.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [01:17<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 1.8635\n",
      "Generated MIDI saved to generated_midi\\generated_0.8.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [01:24<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 1.7697\n",
      "Generated MIDI saved to generated_midi\\generated_0.8.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [01:48<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 1.7099\n",
      "Generated MIDI saved to generated_midi\\generated_0.8.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [02:15<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 1.6780\n",
      "Generated MIDI saved to generated_midi\\generated_0.8.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 24/265 [00:13<02:12,  1.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m train_losses = []\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     epoch_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     scheduler.step()\n\u001b[32m     30\u001b[39m     train_losses.append(epoch_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion)\u001b[39m\n\u001b[32m     13\u001b[39m output = model(src.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m))  \u001b[38;5;66;03m# (seq_len, batch, vocab_size)\u001b[39;00m\n\u001b[32m     15\u001b[39m loss = criterion(output.view(-\u001b[32m1\u001b[39m, tokenizer.vocab_size), tgt.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m optimizer.step()\n\u001b[32m     19\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (src, tgt) in enumerate(tqdm(dataloader)):\n",
    "        src, tgt = src.to(model.device), tgt.to(model.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src.transpose(0, 1))  # (seq_len, batch, vocab_size)\n",
    "        \n",
    "        loss = criterion(output.view(-1, tokenizer.vocab_size), tgt.transpose(0, 1).reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Try to use cuda\n",
    "model.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = train_epoch(model, dataloader, optimizer, criterion)\n",
    "    scheduler.step()\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Saving model epochs\n",
    "    torch.save(model.state_dict(), f\"transformer_epoch_{epoch+1}.pt\")\n",
    "    generate_sample(model, tokenizer, temperature=0.8)\n",
    "\n",
    "# Loss plot\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params\n",
    "\n",
    "-- 1 --\n",
    "\n",
    "d_model = 512\n",
    "\n",
    "n_heads = 8\n",
    "\n",
    "num_layers = 6 \n",
    "\n",
    "d_ff = 2048 \n",
    "\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "seq_len = 512\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of loading and using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-21T06:49:00.333020Z",
     "iopub.status.idle": "2025-06-21T06:49:00.333361Z",
     "shell.execute_reply": "2025-06-21T06:49:00.333216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated MIDI saved to generated_midi\\generated_1.mid\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path):\n",
    "    model = MusicTransformer(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        num_layers=num_layers,\n",
    "        d_ff=d_ff,\n",
    "        dropout=dropout\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Example\n",
    "trained_model = load_model(\"transformer_epoch_1.pt\")\n",
    "generate_sample(trained_model, tokenizer, temperature=1)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1423230,
     "sourceId": 2356848,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
